{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bbec60-7506-46aa-b872-60b3ee8dc247",
   "metadata": {},
   "source": [
    "# Question 1 : What is Ridge Regression, and how does it differ from ordinary least squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074ae05-d1b7-4505-88e5-ccf4d4a5cc8e",
   "metadata": {},
   "source": [
    "# Ans\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c4d50-8bbf-4036-a8a9-cc80b4f00074",
   "metadata": {},
   "source": [
    "Ridge Regression is a regularization technique used in linear regression to mitigate the problems of multicollinearity and overfitting. It differs from ordinary least squares (OLS) regression in its approach to handling these issues.\n",
    "\n",
    "### Ridge Regression:\n",
    "\n",
    "- **Objective**:\n",
    "  - The primary aim of Ridge Regression is to penalize the coefficients by adding a regularization term to the OLS objective function.\n",
    "\n",
    "- **Regularization**:\n",
    "  - Ridge introduces a penalty term to the standard OLS equation, which is the sum of the squares of the coefficients multiplied by a constant (alpha or λ).\n",
    "\n",
    "- **Minimization Objective**:\n",
    "  - The Ridge objective function minimizes the residual sum of squares (RSS) while also considering the penalty term, which is the squared sum of the coefficients, by introducing a trade-off between fitting the data and preventing overfitting.\n",
    "\n",
    "### Differences from Ordinary Least Squares (OLS) Regression:\n",
    "- **Penalty Term:**\n",
    "   - OLS regression seeks to minimize the sum of squared residuals without any penalty term.\n",
    "   - Ridge Regression adds a penalty term to this objective function, penalizing large coefficient values.\n",
    "\n",
    "- **Treatment of Overfitting**:\n",
    "  - OLS aims to minimize the RSS without considering any penalty for the coefficients, making it prone to overfitting with noisy or multicollinear data.\n",
    "  - Ridge adds a regularization term to the OLS equation, preventing overfitting by penalizing large coefficient values, shrinking them towards zero but not exactly to zero.\n",
    "\n",
    "- **Handling Multicollinearity**:\n",
    "  - OLS can be severely impacted by multicollinearity, resulting in unstable coefficient estimates.\n",
    "  - Ridge Regression handles multicollinearity by reducing the impact of highly correlated predictors, stabilizing and improving the robustness of coefficient estimates.\n",
    "\n",
    "- **Impact on Coefficients**:\n",
    "  - OLS estimates can be large, especially with multicollinearity, leading to high variance.\n",
    "  - Ridge constrains the coefficients, reducing their variance and making the model less sensitive to changes in the input data.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "Ridge Regression differs from Ordinary Least Squares regression primarily by introducing a penalty term (L2 norm) to the equation. This term helps prevent overfitting, handles multicollinearity, and stabilizes coefficient estimates by shrinking their values while still retaining all features (albeit with reduced impact). This trade-off between bias and variance is what distinguishes Ridge Regression from the standard OLS approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8c13c-c56f-4b0c-b08e-9be246f1adba",
   "metadata": {},
   "source": [
    "# Question 2 : What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ac364-9405-488d-8d95-4ca7c1bd89f2",
   "metadata": {},
   "source": [
    "# Ans\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cef2d8-56fb-4251-9622-9593d4a27aba",
   "metadata": {},
   "source": [
    "Ridge Regression is an extension of Ordinary Least Squares (OLS) regression that introduces a penalty to the regression coefficients. The assumptions of Ridge Regression are akin to the assumptions in linear regression but also encompass the specifics introduced by the regularization technique. Here are the fundamental assumptions:\n",
    "\n",
    "### Assumptions of Ridge Regression:\n",
    "\n",
    "1. **Linearity**:\n",
    "   - The relationship between the independent variables and the dependent variable should be linear. Ridge Regression, like OLS, assumes a linear relationship between predictors and the target.\n",
    "\n",
    "2. **Independence**:\n",
    "   - The observations in the dataset should be independent of each other. This means that the error terms between observations should not be correlated.\n",
    "\n",
    "3. **Homoscedasticity**:\n",
    "   - The variance of the error terms should be constant across all levels of the independent variables. Ridge Regression, like OLS, assumes constant variance in the errors.\n",
    "\n",
    "4. **Multicollinearity**:\n",
    "   - Ridge Regression assumes that there's no perfect multicollinearity among the independent variables. However, unlike OLS, Ridge Regression is less sensitive to multicollinearity due to its regularization technique.\n",
    "\n",
    "### Additional Assumptions due to Regularization:\n",
    "\n",
    "5. **Existence of Suitable λ (Lambda)**:\n",
    "   - Ridge Regression assumes the availability of an appropriate value for the regularization parameter (λ). The choice of λ impacts the balance between the model's bias and variance.\n",
    "\n",
    "6. **Appropriate Scaling of Variables**:\n",
    "   - Ridge Regression assumes the predictor variables are appropriately standardized or scaled. Scaling ensures that each variable contributes equally to the penalty term, preventing certain variables from dominating the regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1d9b1-58d4-486e-90e3-2ebe30c77798",
   "metadata": {},
   "source": [
    "# Question 3 : How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b64613-8217-4aaf-b9ba-bf6ef6519590",
   "metadata": {},
   "source": [
    "# Ans\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7c0b1-fa64-4168-9153-ec2ada3d38b2",
   "metadata": {},
   "source": [
    "Selecting the appropriate value for the tuning parameter (λ) in Ridge Regression is crucial as it influences the trade-off between model complexity and goodness of fit. Here are some common methods to choose the optimal λ value:\n",
    "\n",
    "### 1. Cross-Validation:\n",
    "\n",
    "- **K-Fold Cross-Validation**:\n",
    "  - Divide the dataset into k subsets/folds.\n",
    "  - Train the model on k-1 subsets and validate on the remaining subset. Repeat this k times, rotating the validation subset.\n",
    "  - Compute the average error across all k iterations for various λ values and select the λ corresponding to the lowest error (e.g., mean squared error or R-squared).\n",
    "\n",
    "### 2. Grid Search:\n",
    "\n",
    "- **Manual Selection via Grid Search**:\n",
    "  - Define a range of λ values to evaluate.\n",
    "  - Train the model using each λ value and measure model performance on a validation set.\n",
    "  - Select the λ that provides the best model performance.\n",
    "\n",
    "### 3. Regularization Path:\n",
    "\n",
    "- **Regularization Path Calculation**:\n",
    "  - Use algorithms like coordinate descent or least-angle regression to calculate the entire path of λ values.\n",
    "  - Identify the λ where the model's performance stabilizes or starts decreasing in terms of error.\n",
    "\n",
    "### 4. Automated Techniques:\n",
    "\n",
    "- **Automated Methods (e.g., Regularization Algorithms)**:\n",
    "  - Use automated techniques that perform internal cross-validation to optimize λ (e.g., scikit-learn's `RidgeCV`).\n",
    "\n",
    "### 5. Information Criteria:\n",
    "\n",
    "- **Information Criteria (e.g., AIC, BIC)**:\n",
    "  - AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) can be used to find the best λ value that minimizes the information criterion.\n",
    "\n",
    "### 6. Domain Knowledge:\n",
    "\n",
    "- **Domain-Specific Knowledge**:\n",
    "  - Consider expert knowledge about the dataset and the problem domain to select an appropriate λ based on the context.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The choice of the tuning parameter (λ) in Ridge Regression is crucial for model performance. Cross-validation, grid search, regularization path methods, automated techniques, information criteria, and domain-specific knowledge are some of the approaches used to select the optimal λ that balances model complexity and performance. The selection method often depends on the dataset size, available computational resources, and the specific goals of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45fc9bd1-fc64-4378-aec8-a0cad037b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lambda (alpha): 0.1\n",
      "Test Score with Best Ridge Model: 0.5758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Load California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a range of λ values for grid search\n",
    "param_grid = {'alpha': [0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "ridge = Ridge()\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best λ value\n",
    "best_lambda = grid_search.best_params_['alpha']\n",
    "\n",
    "# Train Ridge Regression with the best λ\n",
    "best_ridge = Ridge(alpha=best_lambda)\n",
    "best_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "test_score = best_ridge.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Best Lambda (alpha): {best_lambda}\")\n",
    "print(f\"Test Score with Best Ridge Model: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6345f-41ea-4cfa-8c49-0e3cf4ba6832",
   "metadata": {},
   "source": [
    "# Question 4 : Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590cc8f-2635-4f92-8251-18022bc4c301",
   "metadata": {},
   "source": [
    "# Ans\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37df4f-4cc6-4e41-826b-8a04d176f89e",
   "metadata": {},
   "source": [
    "Ridge Regression can assist in feature selection by pushing some coefficients towards zero. Although it doesn't perform variable selection as explicitly as Lasso Regression, Ridge Regression can be employed to identify less important features or reduce the impact of less relevant predictors.\n",
    "\n",
    "### Ridge Regression for Feature Selection:\n",
    "\n",
    "1. **Shrinking Coefficients**:\n",
    "   - Ridge Regression penalizes the sum of squared coefficients (L2 norm) as part of its regularization.\n",
    "   - As the regularization parameter (λ) increases, Ridge Regression shrinks coefficients, leading some towards zero.\n",
    "\n",
    "2. **Reducing Coefficients to Near Zero**:\n",
    "   - While Ridge Regression rarely zeroes out coefficients entirely, it reduces their impact significantly but retains all features in the model.\n",
    "\n",
    "3. **Identifying Less Important Features**:\n",
    "   - Features with coefficients closer to zero in Ridge Regression may be considered less influential.\n",
    "   - By analyzing the magnitude of coefficients, one can identify and focus on features with higher coefficient values.\n",
    "\n",
    "4. **Trade-off between Bias and Variance**:\n",
    "   - Adjusting the regularization parameter (λ) in Ridge Regression can control the trade-off between bias and variance.\n",
    "   - Higher λ values push more coefficients towards zero, aiding in feature selection.\n",
    "\n",
    "### Feature Selection Caveat with Ridge Regression:\n",
    "\n",
    "- Ridge Regression doesn't perform explicit feature selection as Lasso does. It retains all features in the model with reduced impact.\n",
    "- Selecting the right λ value is critical; too low might lead to overfitting, while too high may nullify the effect of the regularization.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Ridge Regression can indirectly assist in feature selection by reducing the impact of less relevant features. While it doesn't outright eliminate features, it mitigates their influence by shrinking coefficients. However, for explicit feature selection, Lasso Regression is generally more effective as it can zero out coefficients, leading to sparser models. The choice between Ridge and Lasso depends on the specific requirements of the analysis, the trade-off between bias and variance, and the balance between model simplicity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e98aa-0e79-4b0c-852c-3669c140a34c",
   "metadata": {},
   "source": [
    "# Question 5 : How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51fc9b-75eb-4b7e-9350-ca01d752b743",
   "metadata": {},
   "source": [
    "# Ans\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d3084-d7a4-40db-8d3d-0bc8cda9f13a",
   "metadata": {},
   "source": [
    "Ridge Regression is particularly effective in handling multicollinearity, which is the presence of high correlation among predictor variables. Here's how Ridge Regression performs in the presence of multicollinearity:\n",
    "\n",
    "### Handling Multicollinearity:\n",
    "\n",
    "1. **Reduction of Coefficient Variance**:\n",
    "   - Ridge Regression works by penalizing the sum of squared coefficients. This penalization reduces the variance of coefficients, making them more stable and less sensitive to variations caused by multicollinearity.\n",
    "\n",
    "2. **Less Impact on Coefficient Estimates**:\n",
    "   - Multicollinearity tends to inflate the variance of coefficient estimates in OLS.\n",
    "   - Ridge Regression constrains these coefficients by reducing their variance, thus providing more reliable estimates in the presence of multicollinearity.\n",
    "\n",
    "3. **Improved Model Stability**:\n",
    "   - High correlation between predictors often causes instability in OLS estimates.\n",
    "   - Ridge Regression, by reducing the sensitivity of coefficients to multicollinearity, provides a more stable model.\n",
    "\n",
    "4. **Better Generalization**:\n",
    "   - Ridge Regression, by stabilizing the coefficients, improves the generalization capability of the model to unseen data.\n",
    "\n",
    "5. **Trade-off between Bias and Variance**:\n",
    "   - Ridge Regression introduces a bias by reducing the magnitude of coefficients; however, it simultaneously reduces the variance that arises due to multicollinearity.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Ridge Regression doesn't eliminate multicollinearity but reduces its impact.\n",
    "- If multicollinearity is extreme, the effectiveness of Ridge Regression might be limited. In such cases, feature selection methods like Lasso might be more appropriate.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Ridge Regression is beneficial in managing multicollinearity by stabilizing the coefficient estimates and reducing their sensitivity to high correlation among predictors. It doesn't eliminate multicollinearity entirely, but it effectively mitigates its impact, leading to a more stable and reliable model, making it a valuable tool in scenarios where multicollinearity is a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c8f1d-4129-4cc8-9729-910fd4474c04",
   "metadata": {},
   "source": [
    "# Question 6 : Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f859d-232f-4f74-977f-292c20c2b01a",
   "metadata": {},
   "source": [
    "# Ans\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c369971-213a-4ad4-a867-72faa7a3198f",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can handle both categorical and continuous independent variables in a regression analysis. However, there are certain considerations and transformations that might be necessary for effective usage of categorical variables within the Ridge Regression framework.\n",
    "\n",
    "### Handling Categorical Variables in Ridge Regression:\n",
    "\n",
    "1. **Encoding Categorical Variables**:\n",
    "   - Categorical variables need to be encoded into a numerical format for use in Ridge Regression.\n",
    "   - One-hot encoding or dummy variable encoding is a common approach to transform categorical variables into numerical format.\n",
    "\n",
    "2. **Creating Dummy Variables**:\n",
    "   - Each category within a categorical variable is represented as a binary dummy variable.\n",
    "   - If a categorical variable has 'k' categories, 'k-1' dummy variables are typically created to avoid multicollinearity.\n",
    "\n",
    "3. **Scaling Variables**:\n",
    "   - Standardizing or scaling continuous and categorical variables before applying Ridge Regression is beneficial. This ensures that all variables have a comparable impact on the regularization.\n",
    "\n",
    "4. **Regularization Across All Variables**:\n",
    "   - Ridge Regression operates on all independent variables, whether they're continuous or categorical, by penalizing the sum of squared coefficients across the entire set of predictors.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Feature Expansion**:\n",
    "  - When using one-hot encoding, this may lead to a large number of variables in the model, potentially requiring careful consideration of the model's complexity and computational resources.\n",
    "\n",
    "- **Multicollinearity**:\n",
    "  - When creating dummy variables for categorical features, it's essential to avoid the \"dummy variable trap,\" which refers to perfect multicollinearity among dummy variables.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Ridge Regression can handle both categorical and continuous variables. To effectively incorporate categorical variables, it's crucial to appropriately encode them into a numerical format and apply necessary transformations while considering the impact on the model's complexity and the potential issues like multicollinearity. Standardizing variables and managing multicollinearity are essential steps for utilizing both categorical and continuous variables within the Ridge Regression framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a77cdf2-3c8f-459a-9a43-22e44996129b",
   "metadata": {},
   "source": [
    "# Question 7 : How do you interpret the coefficients of Ridge Regression ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30d183-1d2d-4e13-8997-149b27020df7",
   "metadata": {},
   "source": [
    "# Ans\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d50e1-b30c-4fb8-9909-6bb3a5b92970",
   "metadata": {},
   "source": [
    "Interpreting coefficients in Ridge Regression follows a similar principle to that of Ordinary Least Squares (OLS) regression, with some considerations due to the regularization introduced by Ridge. The interpretation involves understanding how a unit change in an independent variable affects the dependent variable, taking into account the impact of the penalty term (λ).\n",
    "\n",
    "### Ridge Regression Coefficient Interpretation:\n",
    "\n",
    "1. **Impact on Dependent Variable**:\n",
    "   - Each coefficient in Ridge Regression represents the effect of a one-unit change in the corresponding independent variable while holding other variables constant.\n",
    "   - A positive coefficient indicates a positive relationship with the dependent variable, while a negative coefficient implies a negative relationship.\n",
    "\n",
    "2. **Magnitude of Coefficients**:\n",
    "   - In Ridge Regression, coefficients might be smaller in magnitude due to the penalty term applied to the regression equation.\n",
    "   - The reduction in coefficients' magnitude occurs to prevent overfitting, with some coefficients being shrunk towards zero.\n",
    "\n",
    "3. **Relative Importance**:\n",
    "   - The relative importance of coefficients in Ridge Regression can be gauged by comparing the magnitudes among different predictors. However, it's crucial to consider the impact of scaling on these comparisons.\n",
    "\n",
    "4. **Scaling Impact**:\n",
    "   - Since Ridge Regression uses a penalty term on the sum of squared coefficients, the scale of variables could affect the interpretation of coefficients. Scaling variables might impact the size of coefficients due to their comparability in the penalty term.\n",
    "\n",
    "### Considerations for Interpretation:\n",
    "\n",
    "- Ridge Regression coefficients are interpreted in the context of the penalty introduced by the regularization term.\n",
    "- Comparison of coefficients among predictors is indicative of their relative importance but might be influenced by scaling.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Interpreting coefficients in Ridge Regression involves understanding their impact on the dependent variable in the context of a one-unit change in the independent variables. The coefficients' magnitudes, influenced by the regularization term, provide insights into their relative importance, although the scaling of variables needs to be considered while making comparisons among coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13f79b-413d-4833-9f9b-45c1b51b3585",
   "metadata": {},
   "source": [
    "# Question 8 : Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad036c-64ee-4274-8321-2af137b85ac4",
   "metadata": {},
   "source": [
    "# Ans\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a806c-3f65-4fa8-8dd0-6cc9e42b5f59",
   "metadata": {},
   "source": [
    "Ridge Regression can be applied in time-series data analysis, especially in cases where multicollinearity or overfitting is a concern. However, it's important to note that Ridge Regression, in its standard form, might not fully exploit the temporal nature of time-series data. Still, it can be used in certain aspects:\n",
    "\n",
    "### Usage of Ridge Regression in Time-Series Analysis:\n",
    "\n",
    "1. **Multicollinearity Management**:\n",
    "   - In time-series analysis, some variables might exhibit multicollinearity due to their temporal nature. Ridge Regression can help manage this by reducing the impact of correlated predictors.\n",
    "\n",
    "2. **Regularization for Stability**:\n",
    "   - Ridge Regression introduces regularization, which stabilizes coefficients and prevents overfitting. In time-series analysis, where model stability is crucial, Ridge can be beneficial.\n",
    "\n",
    "3. **Exogenous Variables**:\n",
    "   - When incorporating exogenous variables (variables external to the time series) that might be correlated or cause multicollinearity, Ridge Regression can assist in managing their influence.\n",
    "\n",
    "4. **Control Overfitting**:\n",
    "   - While Ridge might not capture time dependencies inherent in time-series data, it can control overfitting and stabilize the model, especially when dealing with noisy or high-dimensional data.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Temporal Dynamics**:\n",
    "  - Ridge Regression does not inherently capture the sequential nature of time-series data. Time dependencies, trends, and seasonality often need more specialized time-series models.\n",
    "\n",
    "- **Feature Selection**:\n",
    "  - Ridge Regression doesn't explicitly perform feature selection, which could be essential in time-series analysis where identifying important lagged variables is crucial.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Ridge Regression can be used in time-series analysis to manage multicollinearity and prevent overfitting, particularly when dealing with temporal data having multiple correlated predictors. However, in time-series analysis, models explicitly designed to handle temporal dynamics (like ARIMA, SARIMA, or recurrent neural networks) might be more suitable for capturing sequential dependencies inherent in time-series data. While Ridge Regression has its benefits, its application in time-series analysis might require careful consideration of the specific characteristics of the dataset and the nature of temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ddb62-d8c7-491c-a9bf-d88c8f285380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
